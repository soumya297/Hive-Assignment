1.Will the reducer work or not if you use “Limit 1” in any HiveQL query?
Ans:- No, the reducer will not work.

2.Suppose I have installed Apache Hive on top of my Hadoop cluster using default metastore configuration. Then, what will happen if we have multiple clients trying to access Hive at the same time? 
Ans:-
The default metastore configuration allows only one Hive session to be opened at a time for accessing the metastore. Therefore, if multiple clients try to access the metastore at the same time, they will get an error.

3.Suppose, I create a table that contains details of all the transactions done by the customers: CREATE TABLE transaction_details (cust_id INT, amount FLOAT, month STRING, country STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY ‘,’ ;
Now, after inserting 50,000 records in this table, I want to know the total revenue generated for each month. But, Hive is taking too much time in processing this query. How will you solve this problem and list the steps that I will be taking in order to do so?
Ans:-
We can solve this problem of query latency by partitioning the table according to each month. So, for each month we will be scanning only the partitioned data instead of whole data sets.

As we know, we can’t partition an existing non-partitioned table directly. So, we will be taking following steps to solve the very problem: 
i.Create a partitioned table, say partitioned_transaction:
CREATE TABLE partitioned_transaction (cust_id INT, amount FLOAT, country STRING) PARTITIONED BY (month STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY ‘,’ ; 

ii. Enable dynamic partitioning in Hive:
SET hive.exec.dynamic.partition = true;
SET hive.exec.dynamic.partition.mode = nonstrict;
iii. Transfer the data from the non – partitioned table into the newly created partitioned table:
INSERT OVERWRITE TABLE partitioned_transaction PARTITION (month) SELECT cust_id, amount, country, month FROM transaction_details;
iv.Now, we can perform the query using each partition and therefore, decrease the query time.

How can you add a new partition for the month December in the above partitioned table?

4.I am inserting data into a table based on partitions dynamically. But, I received an error – FAILED ERROR IN SEMANTIC ANALYSIS: Dynamic partition strict mode requires at least one static partition column. How will you remove this error?
Ans:-
i need to setup a command i.e.
SET hive.exec.dynamic.partition = true;
SET hive.exec.dynamic.partition.mode = nonstrict;

5.Suppose, I have a CSV file – ‘sample.csv’ present in ‘/temp’ directory with the following entries:
id first_name last_name email gender ip_address
How will you consume this CSV file into the Hive warehouse using built-in SerDe?
Ans:-
Hive provides a specific SerDe for working with CSV files. We can use this SerDe for the sample.csv by issuing following commands:
CREATE EXTERNAL TABLE sample
(id int, first_name string, 
last_name string, email string,
gender string, ip_address string) 
ROW FORMAT SERDE ‘org.apache.hadoop.hive.serde2.OpenCSVSerde’ 
STORED AS TEXTFILE LOCATION ‘/temp’;

6.Suppose, I have a lot of small CSV files present in the input directory in HDFS and I want to create a single Hive table corresponding to these files. The data in these files are in the format: {id, name, e-mail, country}. Now, as we know, Hadoop performance degrades when we use lots of small files.
So, how will you solve this problem where we want to create a single Hive table for lots of small files without degrading the performance of the system?
Ans:-
One can use the SequenceFile format which will group these small files together to form a single sequence file. 
i.Create a temporary table:
CREATE TABLE temp_table (id INT, name STRING, e-mail STRING, country STRING)
ROW FORMAT FIELDS DELIMITED TERMINATED BY ‘,’ STORED AS TEXTFILE;
ii.Load the data into temp_table:
LOAD DATA INPATH ‘/input’ INTO TABLE temp_table;
iii.Create a table that will store data in SequenceFile format:
CREATE TABLE sample_seqfile (id INT, name STRING, e-mail STRING, country STRING)
ROW FORMAT FIELDS DELIMITED TERMINATED BY ‘,’ STORED AS SEQUENCEFILE;
iv.Transfer the data from the temporary table into the sample_seqfile table:
INSERT OVERWRITE TABLE sample SELECT * FROM temp_table;
Hence, a single SequenceFile is generated which contains the data present in all of the input files and therefore, the problem of having lots of small files is finally eliminated.

7.LOAD DATA LOCAL INPATH ‘Home/country/state/’
OVERWRITE INTO TABLE address;

The following statement failed to execute. What can be the cause?
Ans:-
Here the path seems like a directory not a file and data should be a file.

Is it possible to add 100 nodes when we already have 100 nodes in Hive? If yes, how?
Ans:-
Yes, we can add the nodes by following the below steps:

Step 1: Take a new system; create a new username and password
Step 2: Install SSH and with the master node setup SSH connections
Step 3: Add ssh public_rsa id key to the authorized keys file
Step 4: Add the new DataNode hostname, IP address, and other details in /etc/hosts slaves file:

192.168.1.102 slave3.in slave3
Step 5: Start the DataNode on a new node
Step 6: Login to the new node like suhadoop or:

ssh -X hadoop@192.168.1.103
Step 7: Start HDFS of the newly added slave node by using the following command:

./bin/hadoop-daemon.sh start data node
Step 8: Check the output of the jps command on the new node


Hive Join operations

Create a  table named CUSTOMERS(ID | NAME | AGE | ADDRESS   | SALARY)
ans  --
create table customers
(
id int,
name string,
age int,
address string,
salary int
)
row format delimited
feilds terminated by ',';

Create a Second  table ORDER(OID | DATE | CUSTOMER_ID | AMOUNT
ans --
create table orders
(
oid int,
date string,
customer_id int,
amt int
)
row format delimited
feilds terminated by ',';

Now perform different joins operations on top of these tables
(Inner JOIN, LEFT OUTER JOIN ,RIGHT OUTER JOIN ,FULL OUTER JOIN)

# inner join
select * from customer c
inner join orders o
on c.id = o.customer_id

# left join
select * from customer c
left join orders o
on c.id = o.customer_id

# right join
select * from customer c
right join orders o
on c.id = o.customer_id

# full outer join
select * from customer c
full outer join orders o
on c.id = o.customer_id



BUILD A DATA PIPELINE WITH HIVE

Download a data from the given location - 
https://archive.ics.uci.edu/ml/machine-learning-databases/00360/

1. Create a hive table as per given schema in your dataset 
ans --
create table AirQue
(
date_d string,
time string,
CO_GT float,
PT08_S1_CO int,
NMHC_GT int,
C6H6_GT float,
PT08_S2_NMHC int,
NOx_GT int,
PT08_S3_NOx int,
NO2_GT int,
PT08_S4_NO2 int,
PT08_S5_O3 int,
T float,
RH float,
AH float
)
row format delimited
fields terminated by ','
tblproperties("skip.header.line.count"="1");

2. try to place a data into table location
ans -- 
load data inpath '/data_files/AirQualityUCI_Copy.csv' into table AirQue;

3. Perform a select operation.
ans --
select * from AirQue limit 5;
 
4. Fetch the result of the select operation in your local as a csv file.
ans --
created a airque_to_csv.hql and content of this file as below.
use hive_assign_2;
select * from AirQue;

and then ran a command on hive container
hive -f airque_to_csv.hql >> airque_to_csv.csv

 
5. Perform group by operation.
ans -- 
To perform group by operation. I did use aggregate funtion sum.
select date_d,
sum(CO_GT), sum(PT08_S1_CO), sum(NMHC_GT), sum(C6H6_GT),
sum(PT08_S2_NMHC), sum(NOx_GT), sum(PT08_S3_NOx), sum(NO2_GT),
sum(PT08_S4_NO2), sum(PT08_S5_O3), sum(T), sum(RH), sum(AH) from AirQue
group by date_d
 
6. Perform filter operation at least 5 kinds of filter examples.
first filter operation--
select * from AirQue where date_d = '10-03-2004';

second filter operation --
select * from AirQue where date_d = '10-03-2004' and time > '20:00:00';

third filter operation --
select * from AirQue where date_d = '10-03-2004' limit 3;

forth filter operation --
select * from AirQue where date_d in ('10-03-2004', '11-03-2004');

fifth filter operation --
select date_d,count(CO_GT) from AirQue where date_d = '11-03-2004' group by date_d;

 
7. show and example of regex operation
ans --
select distinct(date_d) from AirQue where date_d regexp '^01';

8. alter table operation
ans --
added a column 'myname' with string as datatype
alter table AirQue add columns (myname string);

deleted column 'myname'
we can not just drop a column in hive with the command - alter table table_name drop column_name;
So I used 'replace'

 alter table AirQue replace columns (date_d string,
 time string,
 CO_GT float,
 PT08_S1_CO int,
 NMHC_GT int,
 C6H6_GT float,
 PT08_S2_NMHC int,
 NOx_GT int,
 PT08_S3_NOx int,
 NO2_GT int,
 PT08_S4_NO2 int,
 PT08_S5_O3 int,
 T float,
 RH float,
 AH float);
 
9 . drop table operation
ans --
drop table airque;

10 . order by operation.
ans -- Below is the example of count of negative numbers in all numeric columns from AirQue table and ordered by the count as asc

select 'CO_GT' as word, count(CO_GT) as count from AirQue where CO_GT < 0
union
select 'PT08_S1_CO' as word, count(PT08_S1_CO) as count from AirQue where PT08_S1_CO < 0
union
select 'NMHC_GT' as word, count(NMHC_GT) as count from AirQue where NMHC_GT < 0
union
select 'C6H6_GT' as word, count(C6H6_GT) as count from AirQue where C6H6_GT < 0
union
select 'PT08_S2_NMHC' as word, count(PT08_S2_NMHC) as count from AirQue where PT08_S2_NMHC < 0
union
select 'NOx_GT' as word, count(NOx_GT) as count from AirQue where NOx_GT < 0
union
select 'PT08_S3_NOx' as word, count(PT08_S3_NOx) as count from AirQue where PT08_S3_NOx < 0
union
select 'NO2_GT' as word, count(NO2_GT) as count from AirQue where NO2_GT < 0
union
select 'PT08_S4_NO2' as word, count(PT08_S4_NO2) as count from AirQue where PT08_S4_NO2 < 0
union
select 'PT08_S5_O3' as word, count(PT08_S5_O3) as count from AirQue where PT08_S5_O3 < 0
union
select 'T' as word, count(T) as count from AirQue where T < 0
union
select 'RH' as word, count(RH) as count from AirQue where RH < 0
union
select 'AH' as word, count(AH) as count from AirQue where AH < 0
order by count
 

11 . where clause operations you have to perform.
ans -- 
select * from AirQue where date_d in ('10-03-2004', '11-03-2004');


12 . sorting operation you have to perform.
ans --
select pt08_s1_co from AirQue where date_d = '10-03-2004' sort by pt08_s1_co;


13 . distinct operation you have to perform. 
ans --
select distinct(date_d) from AirQue limit 5;


14 . like an operation you have to perform.
ans --
select * from AirQue where date_d like '01-01%';


15 . union operation you have to perform.
ans -- below is an example of count of negative number in columns CO_GT and PT08_S1_CO from table AirQue.
select 'CO_GT' as word, count(CO_GT) as count from AirQue where CO_GT < 0
union
select 'PT08_S1_CO' as word, count(PT08_S1_CO) as count from AirQue where PT08_S1_CO < 0


16. table view operation you have to perform. 
ans -- below is an example of view for distinct date values
create view unique_dates as
select distinct date_d from AirQue;
